runtime: &runtime
  seed: 42

  tracker_name: null  # wandb project name

  use_cond: false  # load additional condition images
  filter_by_cond: false  # filter data according to condition images
  no_traj: false  # do not load actions data
  empty_prompt: true  # use empty prompt as input
  recon_action: false  # supervise action reconstruction
  multiview: false  # train multivew transformer
  num_observation: 1  # number of input reference images
  visual_guidance: false  # use additional visual conditions
  control_keys: ['depth', 'label']

  push_to_hub: false
  hub_token: null
  logging_dir: logs
  allow_tf32: true
  nccl_timeout: 1800000000
  report_to: null  # wandb

dataset:
  <<: *runtime

  renderings_folder: renderings
  embeddings_folder: embeddings_full

  video_column: video
  latent_column: latent
  depth_column: depth
  semantic_column: semantic

  load_actions: true
  load_tensors: true  # load pre-encoded latents instead of raw data
  load_video: false
  load_condGT: false  # will load conditions from reconstructions

  slice_frame: true
  use_3dvae: true
  vae_has_first_single_frame: true

  bridgev2:
    data_root: ./data/bridge
    num_samples: -1
    camera_ids: [0]
    max_n_view: 1
    action_dim: 7
    sequence_interval: 1
    sequence_length: 16
    sample_frames: 17  # number of frames in video
    start_frame_interval: {'train': 4, 'val': 16, 'test': 16}
    video_size: [320, 480]
    ori_size: [256, 320]
    sample_size: [40, 60]  # `video_size` // vae_scale_factor

    caption_column: texts

  bridgev2_2:
    data_root: ./data/bridgev2
    num_samples: -1
    camera_ids: [0, 1, 2]
    max_n_view: 3
    action_dim: 7
    sequence_interval: 1
    sequence_length: 16
    sample_frames: 17  # number of frames in video
    start_frame_interval: {'train': 4, 'val': 16, 'test': 16}
    video_size: [320, 480]
    ori_size: [256, 320]
    sample_size: [40, 60]  # `video_size` // vae_scale_factor

    caption_column: texts

  droid:
    data_root: ./data/droid
    num_samples: 12e4
    camera_ids: [0, 1]
    max_n_view: 2
    action_dim: 7
    sequence_interval: 3
    sequence_length: 28
    sample_frames: 29  # number of frames in video
    start_frame_interval: {'train': 16, 'val': 72, 'test': 72}
    video_size: [256, 384]
    ori_size: [176, 320]
    sample_size: [32, 48]

    caption_column: language_instruction

  rt1:
    data_root: ./data/rt1
    num_samples: 12e4
    camera_ids: [0]
    max_n_view: 1
    action_dim: 7
    sequence_interval: 2
    sequence_length: 16
    sample_frames: 17  # number of frames in video
    start_frame_interval: {'train': 6, 'val': 16, 'test': 16}
    video_size: [320, 480]
    ori_size: [256, 320]
    sample_size: [40, 60]

    caption_column: texts

vae:
  pretrained_model_name_or_path: THUDM/CogVideoX-2b
  source: cogvideox  # cogvideox, opensora

transformer:
  <<: *runtime
  pretrained_model_name_or_path: THUDM/CogVideoX-2b  # CogVideoX-2b, CogVideoX-5b-I2V, CogVideoX1.5-5b-I2V
  revision: null
  variant: null
  cache_dir: null
  transformer_config_path: null  # only used for from-scratch-training
  transformer_model_name_or_path: null  # only used for from-scratch-training

  guidance_scale: 1

  num_control_blocks: 12
  modulate_encoder_hidden_states: true

  enable_slicing: true
  enable_tiling: true

inference:
  guidance_scale: 1.
  use_dynamic_cfg: false
  enable_model_cpu_offload: false

train:
  <<: *runtime
  output_path: outputs
  output_dir: null  # specified in experiment yamls

  from_scratch: false  # train from scratch
  from_pretrained: true  # finetune from pretrained cogvideox
  overfit: false
  noised_image_dropout: 0.05

  optimizer:
    type: adamw
    use_8bit: false
    use_4bit: false
    use_torchao: false
    beta1: 0.9
    beta2: 0.95
    beta3: null
    prodigy_decouple: false
    weight_decay: 0.001
    epsilon: 1.e-8
    max_grad_norm: 1.
    prodigy_use_bias_correction: false
    prodigy_safeguard_warmup: false
    use_cpu_offload_optimizer: false
    offload_gradients: false

  learning_rate: 2.e-4
  lr_scheduler: cosine_with_restarts
  lr_num_cycles: 1
  lr_warmup_steps: 1000
  lr_power: 1.
  scale_lr: false  # scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size
  max_train_steps: 20000
  num_train_epochs: 1
  mixed_precision: bf16
  gradient_checkpointing: false

  train_batch_size: 4
  dataloader_num_workers: 8
  pin_memory: true
  gradient_accumulation_steps: 4
  validation_steps: 2000
  num_validation_batch: 5  # number of episodes in validation
  num_validation_videos: 1
  checkpointing_steps: 500
  checkpoints_total_limit: 2

  find_unused_parameters: true

  resume_from_checkpoint: latest
