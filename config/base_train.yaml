runtime: &runtime
  seed: 42

  tracker_name: NULL  # wandb project name

  use_cond: False  # load additional condition images
  filter_by_cond: False  # filter data according to condition images
  no_traj: False  # do not load actions data
  empty_prompt: True  # use empty prompt as input
  recon_action: false  # supervise action reconstruction
  multiview: False  # train multivew transformer
  num_observation: 1  # number of input reference images
  visual_guidance: False  # use additional visual conditions
  control_keys: ['depth', 'label']

  push_to_hub: False
  hub_token: NULL
  logging_dir: logs
  allow_tf32: True
  nccl_timeout: 1800000000
  report_to: NULL  # wandb

dataset:
  <<: *runtime

  renderings_folder: renderings
  embeddings_folder: embeddings_full

  video_column: video
  latent_column: latent
  depth_column: depth
  semantic_column: semantic

  load_actions: True
  load_tensors: True  # load pre-encoded latents instead of raw data
  load_video: False
  load_condGT: False  # will load conditions from reconstructions

  slice_frame: True
  use_3dvae: True
  vae_has_first_single_frame: True

  bridgev2:
    data_root: ./data/bridge
    num_samples: -1
    camera_ids: [0]
    max_n_view: 1
    action_dim: 7
    sequence_interval: 1
    sequence_length: 16
    sample_frames: 17  # number of frames in video
    start_frame_interval: {'train': 4, 'val': 16, 'test': 16}
    video_size: [320, 480]
    ori_size: [256, 320]
    sample_size: [40, 60]  # `video_size` // vae_scale_factor

    caption_column: texts

  bridgev2_2:
    data_root: ./data/bridgev2
    num_samples: -1
    camera_ids: [0, 1, 2]
    max_n_view: 3
    action_dim: 7
    sequence_interval: 1
    sequence_length: 16
    sample_frames: 17  # number of frames in video
    start_frame_interval: {'train': 4, 'val': 16, 'test': 16}
    video_size: [320, 480]
    ori_size: [256, 320]
    sample_size: [40, 60]  # `video_size` // vae_scale_factor

    caption_column: texts

  droid:
    data_root: ./data/droid
    num_samples: 12e4
    camera_ids: [0, 1]
    max_n_view: 2
    action_dim: 7
    sequence_interval: 3
    sequence_length: 28
    sample_frames: 29  # number of frames in video
    start_frame_interval: {'train': 16, 'val': 72, 'test': 72}
    video_size: [256, 384]
    ori_size: [176, 320]
    sample_size: [32, 48]

    caption_column: language_instruction

  rt1:
    data_root: ./data/rt1
    num_samples: 12e4
    camera_ids: [0]
    max_n_view: 1
    action_dim: 7
    sequence_interval: 2
    sequence_length: 16
    sample_frames: 17  # number of frames in video
    start_frame_interval: {'train': 6, 'val': 16, 'test': 16}
    video_size: [320, 480]
    ori_size: [256, 320]
    sample_size: [40, 60]

    caption_column: texts

vae:
  pretrained_model_name_or_path: THUDM/CogVideoX-2b
  source: cogvideox  # cogvideox, opensora

transformer:
  <<: *runtime
  pretrained_model_name_or_path: THUDM/CogVideoX-2b  # CogVideoX-2b, CogVideoX-5b-I2V, CogVideoX1.5-5b-I2V
  revision: NULL
  variant: NULL
  cache_dir: NULL
  transformer_config_path: NULL  # only used for from-scratch-training
  transformer_model_name_or_path: NULL  # only used for from-scratch-training

  guidance_scale: 1

  num_control_blocks: 12
  modulate_encoder_hidden_states: True

  enable_slicing: True
  enable_tiling: True

inference:
  guidance_scale: 1.
  use_dynamic_cfg: False
  enable_model_cpu_offload: False

train:
  <<: *runtime
  output_path: outputs
  output_dir: NULL  # specified in experiment yamls

  from_scratch: False  # train from scratch
  from_pretrained: True  # finetune from pretrained cogvideox
  overfit: False
  noised_image_dropout: 0.05

  optimizer:
    type: adamw
    use_8bit: False
    use_4bit: False
    use_torchao: False
    beta1: 0.9
    beta2: 0.95
    beta3: NULL
    prodigy_decouple: False
    weight_decay: 0.001
    epsilon: 1.e-8
    max_grad_norm: 1.
    prodigy_use_bias_correction: False
    prodigy_safeguard_warmup: False
    use_cpu_offload_optimizer: False
    offload_gradients: False

  learning_rate: 2.e-4
  lr_scheduler: cosine_with_restarts
  lr_num_cycles: 1
  lr_warmup_steps: 1000
  lr_power: 1.
  scale_lr: False  # scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size
  max_train_steps: 20000
  num_train_epochs: 1
  mixed_precision: bf16
  gradient_checkpointing: False

  train_batch_size: 4
  dataloader_num_workers: 8
  pin_memory: True
  gradient_accumulation_steps: 4
  validation_steps: 2000
  num_validation_batch: 5  # number of episodes in validation
  num_validation_videos: 1
  checkpointing_steps: 500
  checkpoints_total_limit: 2

  find_unused_parameters: True

  resume_from_checkpoint: latest
